{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CELL ###\n",
    "# # Unsupervised Feature Learning and Classification with a Restricted Boltzmann Machine (RBM)\n",
    "#\n",
    "# This notebook demonstrates how to build and train a Restricted Boltzmann Machine (RBM) from scratch in Python. It uses an RBM to perform **unsupervised feature learning** on the Iris dataset, and then uses the learned features to build a **supervised classification neural network** using logistic regression.\n",
    "#\n",
    "# The project is structured to explain the core concepts of RBMs, including the underlying mathematics, the training algorithm (Contrastive Divergence), and how a trained RBM can be integrated into a supervised classification pipeline.\n",
    "#\n",
    "# ## Preamble: From Neural Networks to Boltzmann Machines\n",
    "#\n",
    "# ### What are Neural Networks?\n",
    "#\n",
    "# A neural network is a computational model inspired by the human brain. It is composed of interconnected nodes, or \"neurons,\" organized in layers. The most basic network consists of an **input layer**, one or more **hidden layers**, and an **output layer**. Each connection between neurons has a numerical weight, and each neuron has a bias.\n",
    "#\n",
    "# Neural networks learn by adjusting these weights and biases. Information flows through the network in a process called **forward propagation**, where the input data is multiplied by the weights, summed, and passed through an **activation function** at each neuron. The network's output is then compared to the desired output, and the error is calculated. This error is then propagated backward through the network (**backpropagation**) to update the weights and biases in a way that reduces the error.\n",
    "#\n",
    "# ### Training vs. Inference\n",
    "#\n",
    "# The process described above is known as **training**. During training, the model learns the patterns and relationships within a dataset. After the model has been trained, it can be used for **inference** or **prediction**. This is the process of using the learned model to make predictions on new, unseen data. In this notebook, our RBM is first trained to learn features from the Iris data in an unsupervised manner (training phase), and then its learned features are used to make predictions with a classifier (inference phase).\n",
    "#\n",
    "# ### What is a Boltzmann Machine?\n",
    "#\n",
    "# A **Boltzmann Machine** is a special type of neural network that differs from traditional feed-forward networks. It's a **generative, stochastic** model.\n",
    "#\n",
    "# * **Generative** means it can learn to model the underlying distribution of the data, allowing it to generate new data samples similar to the training data.\n",
    "#\n",
    "# * **Stochastic** means its neurons make decisions randomly, based on probabilities.\n",
    "#\n",
    "# Boltzmann Machines consist of two types of units: **visible units** ($\\mathbf{v}$) and **hidden units** ($\\mathbf{h}$). The visible units represent the observable data, and the hidden units learn to discover abstract features or patterns in that data. The behavior of the network is defined by an **energy function**, which describes the \"goodness\" of a particular configuration of visible and hidden units.\n",
    "#\n",
    "# The energy function for a general Boltzmann Machine is given by:\n",
    "#\n",
    "# $$E(\\mathbf{v}, \\mathbf{h}) = -\\sum_{i<j} w_{ij} s_i s_j - \\sum_i \\theta_i s_i$$\n",
    "#\n",
    "# Here, $w_{ij}$ are the weights between units $i$ and $j$, and $\\theta_i$ is the bias for unit $i$. The state of each unit $s_i$ is a binary value (0 or 1).\n",
    "#\n",
    "# A **Restricted Boltzmann Machine (RBM)** is a simpler version where there are **no connections between units in the same layer** (visible-visible or hidden-hidden). This \"restriction\" simplifies the model and makes its training significantly more efficient.\n",
    "\n",
    "### CELL ###\n",
    "# ## 1. Setup and Data Preprocessing\n",
    "#\n",
    "# First, we need to load the necessary libraries and the dataset. The Iris dataset is a classic for classification, and we'll use `scikit-learn` to load it and `numpy` for our mathematical operations.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Load and prepare the data ---\n",
    "\n",
    "# Load the Iris dataset from a CSV file\n",
    "# Assuming Iris.csv is in the same directory as the notebook\n",
    "df = pd.read_csv('Iris.csv')\n",
    "\n",
    "# Drop the 'Id' column as it's not needed for the model\n",
    "df = df.drop('Id', axis=1)\n",
    "\n",
    "# Separate features (X) and target labels (y)\n",
    "X = df.iloc[:, 0:4].values\n",
    "y = df.iloc[:, 4].values\n",
    "\n",
    "# --- Data preprocessing ---\n",
    "\n",
    "# Scale the features to a common range.\n",
    "# This is a critical step as RBMs, like many neural networks,\n",
    "# perform better with normalized input data.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the target labels.\n",
    "# This converts categorical labels (e.g., 'Iris-setosa') into a\n",
    "# binary vector format (e.g., [1, 0, 0]).\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = onehot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "### CELL ###\n",
    "# ## 2. The Restricted Boltzmann Machine (RBM)\n",
    "#\n",
    "# An RBM is a type of generative stochastic artificial neural network that can learn a probability distribution over its set of inputs. It is composed of two layers: a **visible layer** (for the input data) and a **hidden layer** (for learning features). The \"restricted\" part means there are no connections between units within the same layer. This architecture simplifies the training process.\n",
    "#\n",
    "# The behavior of an RBM is defined by an **energy function**, which measures how \"bad\" a given configuration of visible and hidden units is.\n",
    "#\n",
    "# ### The Energy Function\n",
    "#\n",
    "# For a given configuration of visible units $\\mathbf{v}$ and hidden units $\\mathbf{h}$, the energy $E(\\mathbf{v}, \\mathbf{h})$ is defined as:\n",
    "#\n",
    "# $$E(\\mathbf{v}, \\mathbf{h}) = -\\sum_{i=1}^{n} b_i v_i - \\sum_{j=1}^{m} c_j h_j - \\sum_{i=1}^{n} \\sum_{j=1}^{m} w_{ij} v_i h_j$$\n",
    "#\n",
    "# Where:\n",
    "# * $v_i$ and $h_j$ are the states of the visible and hidden units.\n",
    "# * $b_i$ and $c_j$ are the biases for the visible and hidden units, respectively.\n",
    "# * $w_{ij}$ is the weight connecting visible unit $i$ to hidden unit $j$.\n",
    "#\n",
    "# The RBM learns by adjusting the weights and biases to assign low energy to the training data.\n",
    "#\n",
    "# ### Conditional Probabilities\n",
    "#\n",
    "# Due to the bipartite graph structure, the visible units are conditionally independent given the hidden units, and vice-versa. This allows us to calculate the probability of a unit's state based on the states of the units in the other layer using the logistic sigmoid function $\\sigma(x) = \\frac{1}{1 + e^{-x}}$.\n",
    "#\n",
    "# The conditional probability of hidden unit $h_j$ being active, given the visible vector $\\mathbf{v}$, is:\n",
    "#\n",
    "# $$P(h_j=1|\\mathbf{v}) = \\sigma(c_j + \\sum_{i=1}^{n} w_{ij} v_i)$$\n",
    "#\n",
    "# Similarly, the conditional probability of visible unit $v_i$ being active, given the hidden vector $\\mathbf{h}$, is:\n",
    "#\n",
    "# $$P(v_i=1|\\mathbf{h}) = \\sigma(b_i + \\sum_{j=1}^{m} w_{ij} h_j)$$\n",
    "#\n",
    "# ### RBM Implementation\n",
    "#\n",
    "# This is a complete implementation of the RBM class from the provided repository.\n",
    "\n",
    "class RBM:\n",
    "    \"\"\"\n",
    "    A Restricted Boltzmann Machine (RBM) implementation from scratch.\n",
    "\n",
    "    An RBM is a two-layer neural network with a visible layer and a hidden layer.\n",
    "    It learns a probability distribution over its inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, visible_dim, hidden_dim, learning_rate=0.01, epochs=100, k=1):\n",
    "        \"\"\"\n",
    "        Initializes the RBM with random weights and biases.\n",
    "\n",
    "        Args:\n",
    "            visible_dim (int): Number of units in the visible layer.\n",
    "            hidden_dim (int): Number of units in the hidden layer.\n",
    "            learning_rate (float): The learning rate for parameter updates.\n",
    "            epochs (int): The number of training epochs.\n",
    "            k (int): The number of Gibbs sampling steps for Contrastive Divergence.\n",
    "        \"\"\"\n",
    "        self.visible_dim = visible_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.k = k\n",
    "\n",
    "        # Initialize weights and biases. Weights are initialized with small\n",
    "        # random values to break symmetry. Biases are initialized to zeros.\n",
    "        self.W = np.random.randn(visible_dim, hidden_dim) * 0.01\n",
    "        self.visible_bias = np.zeros(visible_dim)\n",
    "        self.hidden_bias = np.zeros(hidden_dim)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        The logistic sigmoid activation function.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The sigmoid of the input.\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def _propagate_up(self, v):\n",
    "        \"\"\"\n",
    "        Propagates data from the visible layer to the hidden layer.\n",
    "        This is also known as the 'positive phase' of the RBM.\n",
    "\n",
    "        Args:\n",
    "            v (np.ndarray): The visible layer vector.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the hidden unit probabilities and\n",
    "                   the sampled hidden unit states.\n",
    "        \"\"\"\n",
    "        # Calculate hidden unit probabilities based on the conditional probability formula.\n",
    "        hidden_prob = self._sigmoid(np.dot(v, self.W) + self.hidden_bias)\n",
    "        # Sample the hidden unit states from a Bernoulli distribution.\n",
    "        hidden_states = (hidden_prob > np.random.rand(self.hidden_dim)).astype(int)\n",
    "        return hidden_prob, hidden_states\n",
    "\n",
    "    def _propagate_down(self, h):\n",
    "        \"\"\"\n",
    "        Propagates data from the hidden layer back to the visible layer.\n",
    "        This is also known as the 'negative phase' of the RBM.\n",
    "\n",
    "        Args:\n",
    "            h (np.ndarray): The hidden layer vector.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the visible unit probabilities and\n",
    "                   the sampled visible unit states.\n",
    "        \"\"\"\n",
    "        # Calculate visible unit probabilities based on the conditional probability formula.\n",
    "        visible_prob = self._sigmoid(np.dot(h, self.W.T) + self.visible_bias)\n",
    "        # Sample the visible unit states from a Bernoulli distribution.\n",
    "        visible_states = (visible_prob > np.random.rand(self.visible_dim)).astype(int)\n",
    "        return visible_prob, visible_states\n",
    "\n",
    "    def _free_energy(self, v):\n",
    "        \"\"\"\n",
    "        Calculates the free energy of the visible layer.\n",
    "        The RBM is trained to minimize this value.\n",
    "        \"\"\"\n",
    "        return -np.sum(np.dot(v, self.visible_bias)) - np.sum(np.log(1 + np.exp(np.dot(v, self.W) + self.hidden_bias)))\n",
    "\n",
    "### CELL ###\n",
    "# ## 3. The Training Algorithm: Contrastive Divergence\n",
    "#\n",
    "# Training an RBM means maximizing the likelihood of the training data. This is computationally intractable because it requires summing over all possible configurations of visible and hidden units. To solve this, we use an approximation called **Contrastive Divergence (CD-k)**.\n",
    "#\n",
    "# The core idea of CD-k is to perform a short, approximate Gibbs sampling process instead of a full, converged one. This is sufficient to get a good gradient estimate for training. The process has two phases:\n",
    "#\n",
    "# 1.  **Positive Phase:** Start with a training data vector $\\mathbf{v}^{(0)}$ and propagate it up to the hidden layer to get probabilities $P(\\mathbf{h}|\\mathbf{v}^{(0)})$ and states $\\mathbf{h}^{(0)}$. The goal of this phase is to increase the probability of this real data.\n",
    "# 2.  **Negative Phase:** Propagate the sampled hidden states $\\mathbf{h}^{(0)}$ back down to get a \"reconstruction\" of the visible data $\\mathbf{v}^{(1)}$. Then, propagate this reconstructed visible data back up to the hidden layer to get states $\\mathbf{h}^{(1)}$. This is a single Gibbs sampling step. The goal of this phase is to decrease the probability of this \"fantasy\" data.\n",
    "#\n",
    "# The learning is driven by the difference between the statistics of the positive and negative phases. The update rules for the weights and biases are:\n",
    "#\n",
    "# * **Weight Update:**\n",
    "#     $$\\Delta w_{ij} = \\epsilon (\\langle v_i h_j \\rangle_{\\text{data}} - \\langle v_i h_j \\rangle_{\\text{reconstructed}})$$\n",
    "# * **Visible Bias Update:**\n",
    "#     $$\\Delta b_i = \\epsilon (\\langle v_i \\rangle_{\\text{data}} - \\langle v_i \\rangle_{\\text{reconstructed}})$$\n",
    "# * **Hidden Bias Update:**\n",
    "#     $$\\Delta c_j = \\epsilon (\\langle h_j \\rangle_{\\text{data}} - \\langle h_j \\rangle_{\\text{reconstructed}})$$\n",
    "#\n",
    "# Where $\\epsilon$ is the learning rate, and $\\langle \\cdot \\rangle$ denotes the expectation. We approximate these expectations with the values from our positive and negative phases. The `fit` method in our RBM class implements this logic.\n",
    "\n",
    "def fit(self, X):\n",
    "    \"\"\"\n",
    "    Trains the RBM using the Contrastive Divergence algorithm.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): The training data.\n",
    "    \"\"\"\n",
    "    for epoch in range(self.epochs):\n",
    "        # Mini-batch training for efficiency\n",
    "        for i in range(0, len(X), 1):\n",
    "            v0 = X[i:i+1]\n",
    "\n",
    "            # Positive phase: Propagate real data up to hidden layer\n",
    "            hidden_prob_0, hidden_states_0 = self._propagate_up(v0)\n",
    "\n",
    "            # Negative phase (Gibbs sampling):\n",
    "            # Propagate hidden states down to reconstruct visible states\n",
    "            visible_prob_k, visible_states_k = self._propagate_down(hidden_states_0)\n",
    "            \n",
    "            # This is CD-1. For CD-k, we would run k steps of Gibbs sampling.\n",
    "            # In this implementation, k is fixed at 1.\n",
    "\n",
    "            # Update weights and biases based on the difference\n",
    "            # between positive and negative phase statistics.\n",
    "            # The matrix multiplication `v0.T @ hidden_prob_0` calculates\n",
    "            # the correlation (v_i h_j) for the positive phase.\n",
    "            positive_association = np.dot(v0.T, hidden_prob_0)\n",
    "            \n",
    "            # `visible_prob_k.T @ self._propagate_up(visible_states_k)[0]`\n",
    "            # calculates the correlation for the negative phase.\n",
    "            # This is the reconstructed data's \"fantasy\" correlation.\n",
    "            negative_association = np.dot(visible_prob_k.T, self._propagate_up(visible_states_k)[0])\n",
    "\n",
    "            # Apply the gradient updates\n",
    "            self.W += self.learning_rate * (positive_association - negative_association)\n",
    "            self.visible_bias += self.learning_rate * (np.sum(v0 - visible_prob_k, axis=0))\n",
    "            self.hidden_bias += self.learning_rate * (np.sum(hidden_prob_0 - self._propagate_up(visible_states_k)[0], axis=0))\n",
    "\n",
    "        # Print free energy for monitoring training progress.\n",
    "        # This gives a sense of how well the model is learning the data distribution.\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            free_energy_value = np.mean([self._free_energy(sample) for sample in X])\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, Free Energy: {free_energy_value:.4f}\")\n",
    "\n",
    "### CELL ###\n",
    "# ## 4. The Inference and Classification Pipeline\n",
    "#\n",
    "# Once the RBM is trained, it can be used for various tasks, including dimensionality reduction, feature extraction, and classification. For classification, we don't use the RBM directly. Instead, we use its hidden layer as a feature extractor. The trained RBM has learned meaningful representations of the data in its hidden units. We can then feed these representations into a separate, standard supervised classifier, such as Logistic Regression.\n",
    "#\n",
    "# This is a common \"unsupervised pre-training\" technique where the RBM acts as a feature-learning component before a final supervised classification layer.\n",
    "\n",
    "def _transform(self, v):\n",
    "    \"\"\"\n",
    "    Transforms visible data to hidden layer probabilities.\n",
    "    This is used to get the feature representation for a classifier.\n",
    "\n",
    "    Args:\n",
    "        v (np.ndarray): The visible data to transform.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The hidden layer probabilities.\n",
    "    \"\"\"\n",
    "    return self._sigmoid(np.dot(v, self.W) + self.hidden_bias)\n",
    "\n",
    "# Add the _transform method to the RBM class\n",
    "RBM._transform = _transform\n",
    "\n",
    "### CELL ###\n",
    "# ## 5. Putting It All Together: Main Execution\n",
    "#\n",
    "# Now we combine all the pieces to train the RBM and then classify the Iris data.\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# Define hyperparameters\n",
    "VISIBLE_DIM = X_train.shape[1]\n",
    "HIDDEN_DIM = 20  # A typical choice, can be tuned\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 100\n",
    "\n",
    "print(\"Initializing and training the RBM...\")\n",
    "rbm = RBM(\n",
    "    visible_dim=VISIBLE_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# Train the RBM on the training data.\n",
    "# The RBM learns features from the scaled numerical data.\n",
    "rbm.fit(X_train)\n",
    "\n",
    "# --- Classification with Logistic Regression ---\n",
    "\n",
    "# Use the trained RBM to transform the training and test sets\n",
    "# into a new, higher-level feature space.\n",
    "X_train_rbm = rbm._transform(X_train)\n",
    "X_test_rbm = rbm._transform(X_test)\n",
    "\n",
    "# Now, we use a simple Logistic Regression model for classification\n",
    "print(\"\\nTraining a Logistic Regression model on RBM features...\")\n",
    "# `multi_class='multinomial'` and `solver='lbfgs'` are suitable for multi-class classification\n",
    "classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Train the classifier on the new RBM features and the original labels.\n",
    "classifier.fit(X_train_rbm, y_train)\n",
    "\n",
    "# Make predictions on the test set.\n",
    "y_pred = classifier.predict(X_test_rbm)\n",
    "\n",
    "# Evaluate the final classification accuracy.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nFinal Classification Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
