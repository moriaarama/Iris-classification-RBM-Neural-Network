{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Classification with a Restricted Boltzmann Machine (RBM)\n",
    "\n",
    "## ðŸŒ¸ Overview\n",
    "\n",
    "This notebook demonstrates how to build and train a **Restricted Boltzmann Machine (RBM)** from scratch in Python to perform unsupervised feature learning and then use those features to classify the Iris dataset.\n",
    "\n",
    "The project is structured to explain the core concepts of RBMs, including the underlying mathematics, the training algorithm (Contrastive Divergence), and how a trained RBM can be integrated into a supervised classification pipeline.\n",
    "\n",
    "### Key Features:\n",
    "- âœ… Pure NumPy implementation (no deep learning frameworks)\n",
    "- âœ… Mathematical explanations with equations\n",
    "- âœ… Energy-based learning with Contrastive Divergence\n",
    "- âœ… Unsupervised feature learning + supervised classification\n",
    "- âœ… Before/after training comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble: From Neural Networks to Boltzmann Machines\n",
    "\n",
    "### What are Neural Networks?\n",
    "\n",
    "A neural network is a computational model inspired by the human brain. It is composed of interconnected nodes, or \"neurons,\" organized in layers. The most basic network consists of an **input layer**, one or more **hidden layers**, and an **output layer**. Each connection between neurons has a numerical weight, and each neuron has a bias.\n",
    "\n",
    "Neural networks learn by adjusting these weights and biases. Information flows through the network in a process called **forward propagation**, where the input data is multiplied by the weights, summed, and passed through an **activation function** at each neuron. The network's output is then compared to the desired output, and the error is calculated. This error is then propagated backward through the network (**backpropagation**) to update the weights and biases in a way that reduces the error.\n",
    "\n",
    "### Training vs. Inference\n",
    "\n",
    "The process described above is known as **training**. During training, the model learns the patterns and relationships within a dataset. After the model has been trained, it can be used for **inference** or **prediction**. This is the process of using the learned model to make predictions on new, unseen data. In this notebook, our RBM is first trained to learn features from the Iris data in an unsupervised manner (training phase), and then its learned features are used to make predictions with a classifier (inference phase).\n",
    "\n",
    "### What is a Boltzmann Machine?\n",
    "\n",
    "A **Boltzmann Machine** is a special type of neural network that differs from traditional feed-forward networks. It's a **generative, stochastic** model.\n",
    "\n",
    "* **Generative** means it can learn to model the underlying distribution of the data, allowing it to generate new data samples similar to the training data.\n",
    "* **Stochastic** means its neurons make decisions randomly, based on probabilities.\n",
    "\n",
    "Boltzmann Machines consist of two types of units: **visible units** ($\\mathbf{v}$) and **hidden units** ($\\mathbf{h}$). The visible units represent the observable data, and the hidden units learn to discover abstract features or patterns in that data. The behavior of the network is defined by an **energy function**, which describes the \"goodness\" of a particular configuration of visible and hidden units.\n",
    "\n",
    "The energy function for a general Boltzmann Machine is given by:\n",
    "\n",
    "$E(\\mathbf{v}, \\mathbf{h}) = -\\sum_{i<j} w_{ij} s_i s_j - \\sum_i \\theta_i s_i$\n",
    "\n",
    "Here, $w_{ij}$ are the weights between units $i$ and $j$, and $\\theta_i$ is the bias for unit $i$. The state of each unit $s_i$ is a binary value (0 or 1).\n",
    "\n",
    "A **Restricted Boltzmann Machine (RBM)** is a simpler version where there are **no connections between units in the same layer** (visible-visible or hidden-hidden). This \"restriction\" simplifies the model and makes its training significantly more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š About the Iris Dataset\n",
    "\n",
    "The [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) is one of the most famous datasets in machine learning, introduced by British statistician Ronald Fisher in 1936.\n",
    "\n",
    "**Dataset characteristics:**\n",
    "- **150 samples** of iris flowers\n",
    "- **4 features** for each sample:\n",
    "  - Sepal Length (cm)\n",
    "  - Sepal Width (cm) \n",
    "  - Petal Length (cm)\n",
    "  - Petal Width (cm)\n",
    "- **3 classes** of iris species:\n",
    "  - Iris-setosa\n",
    "  - Iris-versicolor\n",
    "  - Iris-virginica\n",
    "\n",
    "**Goal**: Predict the iris species based on the four flower measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preprocessing\n",
    "\n",
    "First, we need to load the necessary libraries and the dataset. The Iris dataset is a classic for classification, and we'll use `scikit-learn` to load it and `numpy` for our mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ðŸ“¦ Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and prepare the data ---\n",
    "# Load the Iris dataset using sklearn (works well on Kaggle)\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target\n",
    "df['species_name'] = df['species'].map({0: 'Iris-setosa', 1: 'Iris-versicolor', 2: 'Iris-virginica'})\n",
    "\n",
    "# Rename columns for easier access\n",
    "df.columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species_num', 'Species']\n",
    "\n",
    "print(\"ðŸŒ¸ Iris Dataset Overview:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nðŸ“Š Dataset Statistics:\")\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nðŸ·ï¸ Class Distribution:\")\n",
    "print(df['Species'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    row, col = i // 2, i % 2\n",
    "    sns.boxplot(data=df, x='Species', y=feature, ax=axes[row, col])\n",
    "    axes[row, col].set_title(f'Distribution of {feature}', fontsize=12)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“ˆ The boxplots show clear differences between species, making classification feasible!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target labels (y)\n",
    "X = df.iloc[:, 0:4].values\n",
    "y = df.iloc[:, 4].values\n",
    "\n",
    "# --- Data preprocessing ---\n",
    "# Scale the features to a common range.\n",
    "# This is a critical step as RBMs, like many neural networks,\n",
    "# perform better with normalized input data.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the target labels.\n",
    "# This converts categorical labels (e.g., 'Iris-setosa') into a\n",
    "# binary vector format (e.g., [1, 0, 0]).\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = onehot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"ðŸ”¢ Data prepared successfully!\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Features (first sample): {X_train[0]}\")\n",
    "print(f\"Label (first sample): {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Restricted Boltzmann Machine (RBM)\n",
    "\n",
    "An RBM is a type of generative stochastic artificial neural network that can learn a probability distribution over its set of inputs. It is composed of two layers: a **visible layer** (for the input data) and a **hidden layer** (for learning features). The \"restricted\" part means there are no connections between units within the same layer. This architecture simplifies the training process.\n",
    "\n",
    "The behavior of an RBM is defined by an **energy function**, which measures how \"bad\" a given configuration of visible and hidden units is.\n",
    "\n",
    "### The Energy Function\n",
    "\n",
    "For a given configuration of visible units $\\mathbf{v}$ and hidden units $\\mathbf{h}$, the energy $E(\\mathbf{v}, \\mathbf{h})$ is defined as:\n",
    "\n",
    "$E(\\mathbf{v}, \\mathbf{h}) = -\\sum_{i=1}^{n} b_i v_i - \\sum_{j=1}^{m} c_j h_j - \\sum_{i=1}^{n} \\sum_{j=1}^{m} w_{ij} v_i h_j$\n",
    "\n",
    "Where:\n",
    "* $v_i$ and $h_j$ are the states of the visible and hidden units.\n",
    "* $b_i$ and $c_j$ are the biases for the visible and hidden units, respectively.\n",
    "* $w_{ij}$ is the weight connecting visible unit $i$ to hidden unit $j$.\n",
    "\n",
    "The RBM learns by adjusting the weights and biases to assign low energy to the training data.\n",
    "\n",
    "### Conditional Probabilities\n",
    "\n",
    "Due to the bipartite graph structure, the visible units are conditionally independent given the hidden units, and vice-versa. This allows us to calculate the probability of a unit's state based on the states of the units in the other layer using the logistic sigmoid function $\\sigma(x) = \\frac{1}{1 + e^{-x}}$.\n",
    "\n",
    "The conditional probability of hidden unit $h_j$ being active, given the visible vector $\\mathbf{v}$, is:\n",
    "\n",
    "$P(h_j=1|\\mathbf{v}) = \\sigma(c_j + \\sum_{i=1}^{n} w_{ij} v_i)$\n",
    "\n",
    "Similarly, the conditional probability of visible unit $v_i$ being active, given the hidden vector $\\mathbf{h}$, is:\n",
    "\n",
    "$P(v_i=1|\\mathbf{h}) = \\sigma(b_i + \\sum_{j=1}^{m} w_{ij} h_j)$"
   ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "class RBM:\n",
     "    \"\"\"\n",
     "    A Restricted Boltzmann Machine (RBM) implementation from scratch.\n",
     "    An RBM is a two-layer neural network with a visible layer and a hidden layer.\n",
     "    It learns a probability distribution over its inputs.\n",
     "    \"\"\"\n",
     "    def __init__(self, visible_dim, hidden_dim, learning_rate=0.01, epochs=100, k=1):\n",
     "        \"\"\"\n",
     "        Initializes the RBM with random weights and biases.\n",
     "        Args:\n",
     "            visible_dim (int): Number of units in the visible layer.\n",
     "            hidden_dim (int): Number of units in the hidden layer.\n",
     "            learning_rate (float): The learning rate for parameter updates.\n",
     "            epochs (int): The number of training epochs.\n",
     "            k (int): The number of Gibbs sampling steps for Contrastive Divergence.\n",
     "        \"\"\"\n",
     "        self.visible_dim = visible_dim\n",
     "        self.hidden_dim = hidden_dim\n",
     "        self.learning_rate = learning_rate\n",
     "        self.epochs = epochs\n",
     "        self.k = k\n",
     "        \n",
     "        # Initialize weights and biases. Weights are initialized with small\n",
     "        # random values to break symmetry. Biases are initialized to zeros.\n",
     "        self.W = np.random.randn(visible_dim, hidden_dim) * 0.01\n",
     "        self.visible_bias = np.zeros(visible_dim)\n",
     "        self.hidden_bias = np.zeros(hidden_dim)\n",
     "    \n",
     "    def _sigmoid(self, x):\n",
     "        \"\"\"\n",
     "        The logistic sigmoid activation function.\n",
     "        Args:\n",
     "            x (np.ndarray): Input array.\n",
     "        Returns:\n",
     "            np.ndarray: The sigmoid of the input.\n",
     "        \"\"\"\n",
     "        return 1.0 / (1.0 + np.exp(-np.clip(x, -250, 250)))  # Clip to prevent overflow\n",
     "    \n",
     "    def _propagate_up(self, v):\n",
     "        \"\"\"\n",
     "        Propagates data from the visible layer to the hidden layer.\n",
     "        This is also known as the 'positive phase' of the RBM.\n",
     "        Args:\n",
     "            v (np.ndarray): The visible layer vector.\n",
     "        Returns:\n",
     "            tuple: A tuple containing the hidden unit probabilities and\n",
     "                   the sampled hidden unit states.\n",
     "        \"\"\"\n",
     "        # Calculate hidden unit probabilities based on the conditional probability formula.\n",
     "        hidden_prob = self._sigmoid(np.dot(v, self.W) + self.hidden_bias)\n",
     "        # Sample the hidden unit states from a Bernoulli distribution.\n",
     "        hidden_states = (hidden_prob > np.random.rand(self.hidden_dim)).astype(int)\n",
     "        return hidden_prob, hidden_states\n",
     "    \n",
     "    def _propagate_down(self, h):\n",
     "        \"\"\"\n",
     "        Propagates data from the hidden layer back to the visible layer.\n",
     "        This is also known as the 'negative phase' of the RBM.\n",
     "        Args:\n",
     "            h (np.ndarray): The hidden layer vector.\n",
     "        Returns:\n",
     "            tuple: A tuple containing the visible unit probabilities and\n",
     "                   the sampled visible unit states.\n",
     "        \"\"\"\n",
     "        # Calculate visible unit probabilities based on the conditional probability formula.\n",
     "        visible_prob = self._sigmoid(np.dot(h, self.W.T) + self.visible_bias)\n",
     "        # Sample the visible unit states from a Bernoulli distribution.\n",
     "        visible_states = (visible_prob > np.random.rand(self.visible_dim)).astype(int)\n",
     "        return visible_prob, visible_states\n",
     "    \n",
     "    def _free_energy(self, v):\n",
     "        \"\"\"\n",
     "        Calculates the free energy of the visible layer.\n",
     "        The RBM is trained to minimize this value.\n",
     "        \"\"\"\n",
     "        wx_b = np.dot(v, self.W) + self.hidden_bias\n",
     "        visible_bias_term = np.dot(v, self.visible_bias)\n",
     "        hidden_term = np.sum(np.log(1 + np.exp(np.clip(wx_b, -250, 250))))\n",
     "        return -hidden_term - visible_bias_term\n",
     "    \n",
     "    def _transform(self, v):\n",
     "        \"\"\"\n",
     "        Transforms visible data to hidden layer probabilities.\n",
     "        This is used to get the feature representation for a classifier.\n",
     "        Args:\n",
     "            v (np.ndarray): The visible data to transform.\n",
     "        Returns:\n",
     "            np.ndarray: The hidden layer probabilities.\n",
     "        \"\"\"\n",
     "        return self._sigmoid(np.dot(v, self.W) + self.hidden_bias)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 3. The Training Algorithm: Contrastive Divergence\n",
     "\n",
     "Training an RBM means maximizing the likelihood of the training data. This is computationally intractable because it requires summing over all possible configurations of visible and hidden units. To solve this, we use an approximation called **Contrastive Divergence (CD-k)**.\n",
     "\n",
     "The core idea of CD-k is to perform a short, approximate Gibbs sampling process instead of a full, converged one. This is sufficient to get a good gradient estimate for training. The process has two phases:\n",
     "\n",
     "1. **Positive Phase:** Start with a training data vector $\\mathbf{v}^{(0)}$ and propagate it up to the hidden layer to get probabilities $P(\\mathbf{h}|\\mathbf{v}^{(0)})$ and states $\\mathbf{h}^{(0)}$. The goal of this phase is to increase the probability of this real data.\n",
     "2. **Negative Phase:** Propagate the sampled hidden states $\\mathbf{h}^{(0)}$ back down to get a \"reconstruction\" of the visible data $\\mathbf{v}^{(1)}$. Then, propagate this reconstructed visible data back up to the hidden layer to get states $\\mathbf{h}^{(1)}$. This is a single Gibbs sampling step. The goal of this phase is to decrease the probability of this \"fantasy\" data.\n",
     "\n",
     "The learning is driven by the difference between the statistics of the positive and negative phases. The update rules for the weights and biases are:\n",
     "\n",
     "* **Weight Update:**\n",
     "    $$\\Delta w_{ij} = \\epsilon (\\langle v_i h_j \\rangle_{\\text{data}} - \\langle v_i h_j \\rangle_{\\text{reconstructed}})$$\n",
     "* **Visible Bias Update:**\n",
     "    $$\\Delta b_i = \\epsilon (\\langle v_i \\rangle_{\\text{data}} - \\langle v_i \\rangle_{\\text{reconstructed}})$$\n",
     "* **Hidden Bias Update:**\n",
     "    $$\\Delta c_j = \\epsilon (\\langle h_j \\rangle_{\\text{data}} - \\langle h_j \\rangle_{\\text{reconstructed}})$$\n",
     "\n",
     "Where $\\epsilon$ is the learning rate, and $\\langle \\cdot \\rangle$ denotes the expectation."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "def fit(self, X):\n",
     "    \"\"\"\n",
     "    Trains the RBM using the Contrastive Divergence algorithm.\n",
     "    Args:\n",
     "        X (np.ndarray): The training data.\n",
     "    \"\"\"\n",
     "    for epoch in range(self.epochs):\n",
     "        # Mini-batch training for efficiency\n",
     "        for i in range(0, len(X), 1):\n",
     "            v0 = X[i:i+1]\n",
     "            \n",
     "            # Positive phase: Propagate real data up to hidden layer\n",
     "            hidden_prob_0, hidden_states_0 = self._propagate_up(v0)\n",
     "            \n",
     "            # Negative phase (Gibbs sampling):\n",
     "            # Propagate hidden states down to reconstruct visible states\n",
     "            visible_prob_k, visible_states_k = self._propagate_down(hidden_states_0)\n",
     "            \n",
     "            # This is CD-1. For CD-k, we would run k steps of Gibbs sampling.\n",
     "            # Get final hidden probabilities for negative phase\n",
     "            hidden_prob_k, _ = self._propagate_up(visible_states_k)\n",
     "            \n",
     "            # Update weights and biases based on the difference\n",
     "            # between positive and negative phase statistics.\n",
     "            positive_association = np.dot(v0.T, hidden_prob_0)\n",
     "            negative_association = np.dot(visible_prob_k.T, hidden_prob_k)\n",
     "            \n",
     "            # Apply the gradient updates\n",
     "            self.W += self.learning_rate * (positive_association - negative_association)\n",
     "            self.visible_bias += self.learning_rate * (np.sum(v0 - visible_prob_k, axis=0))\n",
     "            self.hidden_bias += self.learning_rate * (np.sum(hidden_prob_0 - hidden_prob_k, axis=0))\n",
     "        \n",
     "        # Print free energy for monitoring training progress.\n",
     "        if (epoch + 1) % 10 == 0:\n",
     "            free_energy_value = np.mean([self._free_energy(sample.reshape(1, -1)) for sample in X])\n",
     "            print(f\"Epoch {epoch + 1}/{self.epochs}, Free Energy: {free_energy_value:.4f}\")\n",
     "\n",
     "# Add the fit method to the RBM class\n",
     "RBM.fit = fit"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 4. Classification Pipeline\n",
     "\n",
     "Once the RBM is trained, it can be used for various tasks, including dimensionality reduction, feature extraction, and classification. For classification, we don't use the RBM directly. Instead, we use its hidden layer as a feature extractor. The trained RBM has learned meaningful representations of the data in its hidden units. We can then feed these representations into a separate, standard supervised classifier, such as Logistic Regression.\n",
     "\n",
     "This is a common \"unsupervised pre-training\" technique where the RBM acts as a feature-learning component before a final supervised classification layer."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# --- Main Execution ---\n",
     "# Define hyperparameters\n",
     "VISIBLE_DIM = X_train.shape[1]\n",
     "HIDDEN_DIM = 20  # A typical choice, can be tuned\n",
     "LEARNING_RATE = 0.1\n",
     "EPOCHS = 100\n",
     "\n",
     "print(\"ðŸ§  Initializing and training the RBM...\")\n",
     "print(f\"Architecture: {VISIBLE_DIM} visible units â†’ {HIDDEN_DIM} hidden units\")\n",
     "print(f\"Learning rate: {LEARNING_RATE}, Epochs: {EPOCHS}\")\n",
     "print()\n",
     "\n",
     "rbm = RBM(\n",
     "    visible_dim=VISIBLE_DIM,\n",
     "    hidden_dim=HIDDEN_DIM,\n",
     "    learning_rate=LEARNING_RATE,\n",
     "    epochs=EPOCHS\n",
     ")\n",
     "\n",
     "# Train the RBM on the training data.\n",
     "# The RBM learns features from the scaled numerical data.\n",
     "rbm.fit(X_train)\n",
     "\n",
     "print(\"\\nâœ… RBM training completed!\")"
    ]
   },
   "source": [
"# --- Classification with Logistic Regression ---\n",
"# Use the trained RBM to transform the training and test sets\n",
"# into a new, higher-level feature space.\n",
"X_train_rbm = rbm._transform(X_train)\n",
"X_test_rbm = rbm._transform(X_test)\n",
"\n",
"print(\"ðŸ”„ Transforming data using trained RBM features...\")\n",
"print(f\"Original features shape: {X_train.shape}\")\n",
"print(f\"RBM features shape: {X_train_rbm.shape}\")\n",
"print()\n",
"\n",
"# Now, we use a simple Logistic Regression model for classification\n",
"print(\"ðŸ“Š Training a Logistic Regression model on RBM features...\")\n",
"classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
"\n",
"# Train the classifier on the new RBM features and the original labels.\n",
"classifier.fit(X_train_rbm, y_train)\n",
"\n",
"# Make predictions on the test set.\n",
"y_pred = classifier.predict(X_test_rbm)\n",
"y_pred_proba = classifier.predict_proba(X_test_rbm)\n",
"\n",
"print(\"âœ… Classification completed!\")\n",
"print()\n",
"\n",
"# --- Evaluation ---\n",
"# Calculate accuracy\n",
"accuracy = accuracy_score(y_test, y_pred)\n",
"print(f\"ðŸŽ¯ Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
"print()\n",
"\n",
"# Detailed classification report\n",
"target_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
"print(\"ðŸ“‹ Classification Report:\")\n",
"print(classification_report(y_test, y_pred, target_names=target_names))\n",
"\n",
"# Confusion Matrix\n",
"cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
"print(\"\\nðŸ”¢ Confusion Matrix:\")\n",
"print(cm)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# --- Baseline Comparison ---\n",
"print(\"\\nðŸ” Baseline Comparison (Logistic Regression without RBM):\")\n",
"\n",
"# Train a baseline classifier directly on the original features\n",
"baseline_classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
"baseline_classifier.fit(X_train, y_train)\n",
"y_pred_baseline = baseline_classifier.predict(X_test)\n",
"\n",
"baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
"print(f\"ðŸ“Š Baseline Accuracy (without RBM): {baseline_accuracy:.4f} ({baseline_accuracy*100:.2f}%)\")\n",
"print(f\"ðŸ§  RBM-enhanced Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
"\n",
"improvement = accuracy - baseline_accuracy\n",
"print(f\"ðŸ“ˆ Improvement: {improvement:+.4f} ({improvement*100:+.2f} percentage points)\")\n",
"\n",
"if improvement > 0:\n",
"    print(\"âœ… RBM features improved classification performance!\")\n",
"elif improvement < 0:\n",
"    print(\"âš ï¸ RBM features decreased performance. Consider hyperparameter tuning.\")\n",
"else:\n",
"    print(\"âž– RBM features showed no improvement over baseline.\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 5. Visualization of Results\n",
"\n",
"Let's visualize the confusion matrix and compare the feature representations learned by the RBM."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Visualize confusion matrix\n",
"plt.figure(figsize=(12, 5))\n",
"\n",
"# RBM-enhanced classifier confusion matrix\n",
"plt.subplot(1, 2, 1)\n",
"sns.heatmap(confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1)), \n",
"            annot=True, fmt='d', cmap='Blues',\n",
"            xticklabels=target_names,\n",
"            yticklabels=target_names)\n",
"plt.title(f'RBM-Enhanced Classifier\\nAccuracy: {accuracy:.3f}')\n",
"plt.ylabel('True Label')\n",
"plt.xlabel('Predicted Label')\n",
"\n",
"# Baseline classifier confusion matrix\n",
"plt.subplot(1, 2, 2)\n",
"sns.heatmap(confusion_matrix(y_test.argmax(axis=1), y_pred_baseline.argmax(axis=1)), \n",
"            annot=True, fmt='d', cmap='Reds',\n",
"            xticklabels=target_names,\n",
"            yticklabels=target_names)\n",
"plt.title(f'Baseline Classifier\\nAccuracy: {baseline_accuracy:.3f}')\n",
"plt.ylabel('True Label')\n",
"plt.xlabel('Predicted Label')\n",
"\n",
"plt.tight_layout()\n",
"plt.show()"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Visualize RBM learned features\n",
"plt.figure(figsize=(15, 5))\n",
"\n",
"# Original features scatter plot\n",
"plt.subplot(1, 3, 1)\n",
"colors = ['red', 'green', 'blue']\n",
"species_labels = y_test.argmax(axis=1)\n",
"for i, species in enumerate(target_names):\n",
"    mask = species_labels == i\n",
"    plt.scatter(X_test[mask, 0], X_test[mask, 1], \n",
"                c=colors[i], label=species, alpha=0.7)\n",
"plt.xlabel('Sepal Length (scaled)')\n",
"plt.ylabel('Sepal Width (scaled)')\n",
"plt.title('Original Features\\n(First 2 dimensions)')\n",
"plt.legend()\n",
"plt.grid(True, alpha=0.3)\n",
"\n",
"# RBM features scatter plot (first 2 hidden units)\n",
"plt.subplot(1, 3, 2)\n",
"for i, species in enumerate(target_names):\n",
"    mask = species_labels == i\n",
"    plt.scatter(X_test_rbm[mask, 0], X_test_rbm[mask, 1], \n",
"                c=colors[i], label=species, alpha=0.7)\n",
"plt.xlabel('RBM Feature 1')\n",
"plt.ylabel('RBM Feature 2')\n",
"plt.title('RBM Features\\n(First 2 hidden units)')\n",
"plt.legend()\n",
"plt.grid(True, alpha=0.3)\n",
"\n",
"# Feature importance (RBM weights)\n",
"plt.subplot(1, 3, 3)\n",
"feature_importance = np.abs(rbm.W).mean(axis=1)\n",
"feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
"plt.bar(feature_names, feature_importance)\n",
"plt.title('RBM Feature Importance\\n(Average absolute weights)')\n",
"plt.ylabel('Importance')\n",
"plt.xticks(rotation=45)\n",
"\n",
"plt.tight_layout()\n",
"plt.show()\n",
"\n",
"print(\"ðŸ“Š Visualization shows:\")\n",
"print(\"   â€¢ Original features: Raw measurements\")\n",
"print(\"   â€¢ RBM features: Learned abstract representations\")\n",
"print(\"   â€¢ Feature importance: Which original features the RBM relies on most\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 6. Understanding the RBM's Learned Representations\n",
"\n",
"Let's examine what the RBM has learned by looking at its weights and reconstructions."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Analyze RBM weights\n",
"print(\"ðŸ” RBM Analysis:\")\n",
"print(f\"Weight matrix shape: {rbm.W.shape}\")\n",
"print(f\"Weight statistics:\")\n",
"print(f\"   Mean: {rbm.W.mean():.4f}\")\n",
"print(f\"   Std:  {rbm.W.std():.4f}\")\n",
"print(f\"   Min:  {rbm.W.min():.4f}\")\n",
"print(f\"   Max:  {rbm.W.max():.4f}\")\n",
"print()\n",
"\n",
"# Test reconstruction capability\n",
"print(\"ðŸ”„ Testing RBM reconstruction capability:\")\n",
"test_sample = X_test[0:1]  # Take first test sample\n",
"print(f\"Original sample: {test_sample[0]}\")\n",
"\n",
"# Forward pass: visible -> hidden\n",
"hidden_prob, hidden_states = rbm._propagate_up(test_sample)\n",
"print(f\"Hidden activations: {hidden_prob[0][:5]}... (showing first 5)\")\n",
"\n",
"# Backward pass: hidden -> visible (reconstruction)\n",
"reconstructed_prob, reconstructed_states = rbm._propagate_down(hidden_states)\n",
"print(f\"Reconstructed sample: {reconstructed_prob[0]}\")\n",
"\n",
"# Calculate reconstruction error\n",
"reconstruction_error = np.mean((test_sample - reconstructed_prob) ** 2)\n",
"print(f\"Reconstruction error (MSE): {reconstruction_error:.4f}\")\n",
"\n",
"if reconstruction_error < 0.1:\n",
"    print(\"âœ… Good reconstruction! RBM learned meaningful features.\")\n",
"elif reconstruction_error < 0.5:\n",
"    print(\"âš ï¸ Moderate reconstruction. RBM partially learned the data.\")\n",
"else:\n",
"    print(\"âŒ Poor reconstruction. Consider adjusting hyperparameters.\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 7. Hyperparameter Sensitivity Analysis\n",
"\n",
"Let's briefly explore how different hyperparameters affect the RBM's performance."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Quick hyperparameter sensitivity test\n",
"print(\"ðŸ”¬ Hyperparameter Sensitivity Analysis:\")\n",
"print(\"Testing different numbers of hidden units...\")\n",
"\n",
"hidden_dims = [5, 10, 15, 20, 30]\n",
"accuracies = []\n",
"\n",
"for hidden_dim in hidden_dims:\n",
"    # Train a smaller RBM with fewer epochs for speed\n",
"    test_rbm = RBM(\n",
"        visible_dim=VISIBLE_DIM,\n",
"        hidden_dim=hidden_dim,\n",
"        learning_rate=0.1,\n",
"        epochs=50  # Reduced for speed\n",
"    )\n",
"    \n",
"    test_rbm.fit(X_train)\n",
"    \n",
"    # Transform and classify\n",
"    X_train_transformed = test_rbm._transform(X_train)\n",
"    X_test_transformed = test_rbm._transform(X_test)\n",
"    \n",
"    test_classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
"    test_classifier.fit(X_train_transformed, y_train)\n",
"    test_pred = test_classifier.predict(X_test_transformed)\n",
"    \n",
"    test_accuracy = accuracy_score(y_test, test_pred)\n",
"    accuracies.append(test_accuracy)\n",
"    \n",
"    print(f\"   Hidden units: {hidden_dim:2d} â†’ Accuracy: {test_accuracy:.3f}\")\n",
"\n",
"# Plot results\n",
"plt.figure(figsize=(10, 6))\n",
"plt.plot(hidden_dims, accuracies, 'bo-', linewidth=2, markersize=8)\n",
"plt.axhline(y=baseline_accuracy, color='red', linestyle='--', label=f'Baseline: {baseline_accuracy:.3f}')\n",
"plt.xlabel('Number of Hidden Units')\n",
"plt.ylabel('Test Accuracy')\n",
"plt.title('RBM Performance vs Hidden Layer Size')\n",
"plt.grid(True, alpha=0.3)\n",
"plt.legend()\n",
"plt.ylim(0.8, 1.0)\n",
"\n",
"for i, (hd, acc) in enumerate(zip(hidden_dims, accuracies)):\n",
"    plt.annotate(f'{acc:.3f}', (hd, acc), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
"\n",
"plt.show()\n",
"\n",
"best_hidden_dim = hidden_dims[np.argmax(accuracies)]\n",
"best_accuracy = max(accuracies)\n",
"print(f\"\\nðŸ† Best configuration: {best_hidden_dim} hidden units â†’ {best_accuracy:.3f} accuracy\")"
]
},
{
    "cell_type": "markdown",
    "metadata": {},
    "source": [
    "## 8. Conclusions and Key Takeaways\n",
    "\n",
    "### ðŸŽ¯ What We've Accomplished\n",
    "\n",
    "In this notebook, we successfully:\n",
    "\n",
    "1. **Built an RBM from scratch** using only NumPy, without relying on deep learning frameworks\n",
    "2. **Implemented Contrastive Divergence** training algorithm with mathematical rigor\n",
    "3. **Applied unsupervised feature learning** to extract meaningful representations from the Iris dataset\n",
    "4. **Integrated RBM features** into a supervised classification pipeline\n",
    "5. **Compared performance** against baseline methods and analyzed hyperparameter sensitivity\n",
    "\n",
    "### ðŸ§  Key Insights About RBMs\n",
    "\n",
    "**Strengths:**\n",
    "- âœ… **Unsupervised learning**: Can discover hidden patterns without labeled data\n",
    "- âœ… **Feature extraction**: Learns abstract representations that can improve classification\n",
    "- âœ… **Generative modeling**: Can generate new samples similar to training data\n",
    "- âœ… **Dimensionality transformation**: Can map data to higher or lower dimensions\n",
    "\n",
    "**Limitations:**\n",
    "- âŒ **Training complexity**: Requires careful hyperparameter tuning\n",
    "- âŒ **Computational cost**: Can be slow for large datasets\n",
    "- âŒ **Binary assumption**: Works best with binary or bounded continuous data\n",
    "- âŒ **Local optima**: May get stuck in suboptimal solutions\n",
    "\n",
    "### ðŸ“Š Performance Analysis\n",
    "\n",
    "The results show that RBMs can be effective for feature learning, particularly when:\n",
    "- The dataset has underlying structure that can be captured by hidden units\n",
    "- Hyperparameters are properly tuned (hidden layer size, learning rate, epochs)\n",
    "- The original features benefit from non-linear transformation\n",
    "\n",
    "### ðŸ”¬ When to Use RBMs\n",
    "\n",
    "**Good use cases:**\n",
    "- **Dimensionality reduction** for visualization or preprocessing\n",
    "- **Feature learning** for datasets with complex, non-linear patterns\n",
    "- **Anomaly detection** by measuring reconstruction error\n",
    "- **Data generation** for augmenting small datasets\n",
    "- **Pre-training** for deep neural networks (historically important)\n",
    "\n",
    "**Consider alternatives when:**\n",
    "- Dataset is very large (consider autoencoders or variational autoencoders)\n",
    "- Linear methods work well (PCA, linear classifiers)\n",
    "- Computational resources are limited\n",
    "- Interpretability is crucial (consider simpler methods)\n",
    "\n",
    "### ðŸš€ Extensions and Future Work\n",
    "\n",
    "**Possible improvements:**\n",
    "1. **Gaussian-Bernoulli RBMs** for continuous data\n",
    "2. **Deep Belief Networks** by stacking multiple RBMs\n",
    "3. **Conditional RBMs** for incorporating label information\n",
    "4. **Regularization techniques** to prevent overfitting\n",
    "5. **Advanced sampling methods** beyond Contrastive Divergence\n",
    "\n",
    "**Modern alternatives:**\n",
    "- **Variational Autoencoders (VAEs)** for better generative modeling\n",
    "- **Transformer-based models** for sequence data\n",
    "- **Graph Neural Networks** for structured data\n",
    "- **Self-supervised learning** methods\n"
    ]
    },
    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
    "## 9. Mathematical Appendix\n",
    "\n",
    "### Energy Function Derivation\n",
    "\n",
    "The probability of a joint configuration $(\\mathbf{v}, \\mathbf{h})$ follows the Boltzmann distribution:\n",
    "\n",
    "$$P(\\mathbf{v}, \\mathbf{h}) = \\frac{1}{Z} e^{-E(\\mathbf{v}, \\mathbf{h})}$$\n",
    "\n",
    "Where $Z$ is the partition function:\n",
    "\n",
    "$$Z = \\sum_{\\mathbf{v}, \\mathbf{h}} e^{-E(\\mathbf{v}, \\mathbf{h})}$$\n",
    "\n",
    "### Conditional Probability Derivation\n",
    "\n",
    "Due to the bipartite structure, we can derive:\n",
    "\n",
    "$$P(h_j = 1|\\mathbf{v}) = \\frac{P(h_j = 1, \\mathbf{v})}{P(\\mathbf{v})} = \\frac{\\sum_{\\mathbf{h}_{-j}} P(h_j = 1, \\mathbf{h}_{-j}, \\mathbf{v})}{\\sum_{\\mathbf{h}} P(\\mathbf{h}, \\mathbf{v})}$$\n",
    "\n",
    "After algebraic manipulation, this simplifies to:\n",
    "\n",
    "$$P(h_j = 1|\\mathbf{v}) = \\sigma\\left(c_j + \\sum_i W_{ij} v_i\\right)$$\n",
    "\n",
    "### Contrastive Divergence Gradient\n",
    "\n",
    "The log-likelihood gradient with respect to weights is:\n",
    "\n",
    "$$\\frac{\\partial \\log P(\\mathbf{v}_{\\text{data}})}{\\partial W_{ij}} = \\langle v_i h_j \\rangle_{\\text{data}} - \\langle v_i h_j \\rangle_{\\text{model}}$$\n",
    "\n",
    "Contrastive Divergence approximates the model expectation with a short Gibbs chain:\n",
    "\n",
    "$$\\langle v_i h_j \\rangle_{\\text{model}} \\approx \\langle v_i h_j \\rangle_{\\text{CD-k}}$$\n"
    ]
    },
    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
    "## 10. References and Further Reading\n",
    "\n",
    "### ðŸ“š Foundational Papers\n",
    "\n",
    "1. **Hinton, G. E. (2002)**. \"Training products of experts by minimizing contrastive divergence.\" *Neural Computation*, 14(8), 1771-1800.\n",
    "   - The seminal paper introducing Contrastive Divergence\n",
    "\n",
    "2. **Hinton, G. E., & Salakhutdinov, R. R. (2006)**. \"Reducing the dimensionality of data with neural networks.\" *Science*, 313(5786), 504-507.\n",
    "   - Shows how RBMs can be used for deep learning and dimensionality reduction\n",
    "\n",
    "3. **Fischer, A., & Igel, C. (2012)**. \"An introduction to restricted Boltzmann machines.\" *Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications*, 14-36.\n",
    "   - Comprehensive tutorial on RBM theory and applications\n",
    "\n",
    "### ðŸ”— Additional Resources\n",
    "\n",
    "- **Deep Learning Book** by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (Chapter 20)\n",
    "- **Pattern Recognition and Machine Learning** by Christopher Bishop (Chapter 8)\n",
    "- **Coursera Deep Learning Specialization** by Andrew Ng\n",
    "- **Scholarpedia article on Boltzmann Machines** by Geoffrey Hinton\n",
    "\n",
    "### ðŸ’» Code Resources\n",
    "\n",
    "- **scikit-learn**: For baseline machine learning algorithms\n",
    "- **TensorFlow/PyTorch**: For modern deep learning implementations\n",
    "- **Original Hinton MATLAB code**: Available in some academic repositories\n",
    "\n",
    "### ðŸŽ“ Learning Path\n",
    "\n",
    "1. **Prerequisites**: Linear algebra, probability theory, basic machine learning\n",
    "2. **Next steps**: Variational autoencoders, deep belief networks, modern generative models\n",
    "3. **Advanced topics**: Energy-based models, normalizing flows, diffusion models\n"
    ]
    },
    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've successfully implemented and understood a **Restricted Boltzmann Machine** from scratch! \n",
    "\n",
    "### What you've learned:\n",
    "- âœ… **Energy-based modeling** and probabilistic neural networks\n",
    "- âœ… **Contrastive Divergence** training algorithm\n",
    "- âœ… **Unsupervised feature learning** for classification\n",
    "- âœ… **Mathematical foundations** of generative models\n",
    "- âœ… **Practical implementation** without deep learning frameworks\n",
    "\n",
    "### Next challenges:\n",
    "- ðŸš€ Implement **Deep Belief Networks** by stacking RBMs\n",
    "- ðŸš€ Try **Variational Autoencoders** for more modern generative modeling\n",
    "- ðŸš€ Explore **Graph Neural Networks** for structured data\n",
    "- ðŸš€ Apply RBMs to **image data** (MNIST, CIFAR-10)\n",
    "\n",
    "**Happy learning!** ðŸ§ âœ¨\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates educational concepts. For production use, consider optimized implementations with modern frameworks.*"
    ]
    }